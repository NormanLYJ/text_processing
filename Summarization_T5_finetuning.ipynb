{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"TPML_4_5_Summarization_T5_finetuning.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"widgets":{"application/vnd.jupyter.widget-state+json":{"b0b20bf692004a19b33a2971502eb448":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_dcb6aa15ba4b4718b7f60728d5a83ba1","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_9679bf8840b247a984af62b6c64caa15","IPY_MODEL_949edc33c0b548d0b16ffe7122c46721"]}},"dcb6aa15ba4b4718b7f60728d5a83ba1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9679bf8840b247a984af62b6c64caa15":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_d7986cb8f5d34c7ea66d1a45a22d9ee8","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":791656,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":791656,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_fa5159bd32cd42dcac3bb73e94ff6186"}},"949edc33c0b548d0b16ffe7122c46721":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_843f88f818494abaa40772f3e3ee3dfb","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 792k/792k [02:31&lt;00:00, 5.22kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_569c847de00b4a89a65ceda90782249f"}},"d7986cb8f5d34c7ea66d1a45a22d9ee8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"fa5159bd32cd42dcac3bb73e94ff6186":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"843f88f818494abaa40772f3e3ee3dfb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"569c847de00b4a89a65ceda90782249f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"WJcfQqa3SddR"},"source":["# Fine Tuning T5 for Summarization"]},{"cell_type":"markdown","metadata":{"id":"FQamHb4aSddS"},"source":["\n","### Introduction\n","\n","In this tutorial we will be fine tuning a transformer model for **Summarization Task**. \n","In this task a summary of a given article/document is generated when passed through a network. There are 2 types of summary generation mechanisms:\n","\n","1. ***Extractive Summary:*** the network calculates the most important sentences from the article and gets them together to provide the most meaningful information from the article.\n","2. ***Abstractive Summary***: The network creates new sentences to encapsulate maximum gist of the article and generates that as output. The sentences in the summary may or may not be contained in the article. \n","\n","In this tutorial we will be generating ***Abstractive Summary***. \n","\n","#### Flow of the notebook\n","\n","The notebook will be divided into separate sections:\n","\n","1. [Preparing Environment and Importing Libraries](#section01)\n","2. [Preparing the Dataset for data processing: Class](#section02)\n","3. [Fine Tuning the Model: Function](#section03)\n","4. [Validating the Model Performance: Function](#section04)\n","5. [The Main Process](#section05)\n","    * [Importing and Pre-Processing the domain data](#section502)\n","    * [Creation of Dataset and Dataloader](#section503)\n","    * [Neural Network and Optimizer](#section504)\n","    * [Training Model](#section505)\n","    * [Validation and generation of Summary](#section506)\n","6. [Examples of the Summary Generated from the model](#section06)\n","\n","\n","#### Data:\n","* We are using the News Summary dataset available at [Kaggle](https://www.kaggle.com/sunnysai12345/news-summary)\n","* This dataset is the collection created from Newspapers published in India, extracting, details that are listed below.  We are referring only to the first csv file from the data dump: `news_summary.csv`\n","* There are`4514` rows of data.  Where each row has the following data-point:\n","  - **author** : Author of the article\n","  - **date** : Date the article was published\n","  - **headline**: Headline for the published article\n","  - **read_more** : URL for the article to follow online\n","  - **text**: This is the summary of the article\n","  - **ctext**: This is the complete article\n","\n","\n","####Language Model Used: \n","- This notebook uses one of the most recent and novel transformers model ***T5***. [Research Paper](https://arxiv.org/abs/1910.10683)    \n","- **Text-2-Text** - According to the graphic taken from the T5 paper. All NLP tasks are converted to a **text-to-text** problem. Tasks such as translation, classification, summarization and question answering, all of them are treated as a text-to-text conversion problem, rather than seen as separate unique problem statements.\n","- **Unified approach for NLP Deep Learning** - Since the task is reflected purely in the text input and output, you can use the same model, objective, training procedure, and decoding process to ANY task. Above framework can be used for any task - show Q&A, summarization, etc. \n","\n","\n","![**Each NLP problem as a “text-to-text” problem** - input: text, output: text](https://miro.medium.com/max/4006/1*D0J1gNQf8vrrUpKeyD8wPA.png) \n","\t    \n","\n","- **Script Objective**:\n","\t- The objective of this script is to fine tune ***T5 *** to be able to generate summary, that a close to or better than the actual summary  while ensuring the important information from the article is not lost.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"juAmie22SddS"},"source":["<a id='section01'></a>\n","### Preparing Environment and Importing Libraries\n","\n","At this step we will be installing the necessary libraries followed by importing the libraries and modules needed to run our script. \n","We will be installing:\n","* transformers\n","* SentencePiece (required by T5)\n","\n","Libraries imported are:\n","* Pandas\n","* Pytorch\n","* Pytorch Utils for Dataset and Dataloader\n","* Transformers\n","* T5 Model and Tokenizer\n","\n","Then we will check the GPU avaiable to us, using the nvidia command followed by defining our device."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WD_vnyLXZQzD","outputId":"85c84ceb-ad9a-47f7-9635-7cf4f1b48e9b"},"source":["!pip install transformers \n","!pip install SentencePiece \n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/40/866cbfac4601e0f74c7303d533a9c5d4a53858bd402e08e3e294dd271f25/transformers-4.2.1-py3-none-any.whl (1.8MB)\n","\r\u001b[K     |▏                               | 10kB 19.5MB/s eta 0:00:01\r\u001b[K     |▍                               | 20kB 24.4MB/s eta 0:00:01\r\u001b[K     |▋                               | 30kB 18.1MB/s eta 0:00:01\r\u001b[K     |▊                               | 40kB 15.9MB/s eta 0:00:01\r\u001b[K     |█                               | 51kB 10.8MB/s eta 0:00:01\r\u001b[K     |█▏                              | 61kB 12.4MB/s eta 0:00:01\r\u001b[K     |█▎                              | 71kB 11.3MB/s eta 0:00:01\r\u001b[K     |█▌                              | 81kB 12.2MB/s eta 0:00:01\r\u001b[K     |█▊                              | 92kB 11.0MB/s eta 0:00:01\r\u001b[K     |█▉                              | 102kB 10.2MB/s eta 0:00:01\r\u001b[K     |██                              | 112kB 10.2MB/s eta 0:00:01\r\u001b[K     |██▎                             | 122kB 10.2MB/s eta 0:00:01\r\u001b[K     |██▍                             | 133kB 10.2MB/s eta 0:00:01\r\u001b[K     |██▋                             | 143kB 10.2MB/s eta 0:00:01\r\u001b[K     |██▉                             | 153kB 10.2MB/s eta 0:00:01\r\u001b[K     |███                             | 163kB 10.2MB/s eta 0:00:01\r\u001b[K     |███▏                            | 174kB 10.2MB/s eta 0:00:01\r\u001b[K     |███▍                            | 184kB 10.2MB/s eta 0:00:01\r\u001b[K     |███▌                            | 194kB 10.2MB/s eta 0:00:01\r\u001b[K     |███▊                            | 204kB 10.2MB/s eta 0:00:01\r\u001b[K     |████                            | 215kB 10.2MB/s eta 0:00:01\r\u001b[K     |████                            | 225kB 10.2MB/s eta 0:00:01\r\u001b[K     |████▎                           | 235kB 10.2MB/s eta 0:00:01\r\u001b[K     |████▌                           | 245kB 10.2MB/s eta 0:00:01\r\u001b[K     |████▋                           | 256kB 10.2MB/s eta 0:00:01\r\u001b[K     |████▉                           | 266kB 10.2MB/s eta 0:00:01\r\u001b[K     |█████                           | 276kB 10.2MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 286kB 10.2MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 296kB 10.2MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 307kB 10.2MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 317kB 10.2MB/s eta 0:00:01\r\u001b[K     |██████                          | 327kB 10.2MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 337kB 10.2MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 348kB 10.2MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 358kB 10.2MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 368kB 10.2MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 378kB 10.2MB/s eta 0:00:01\r\u001b[K     |███████                         | 389kB 10.2MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 399kB 10.2MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 409kB 10.2MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 419kB 10.2MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 430kB 10.2MB/s eta 0:00:01\r\u001b[K     |████████                        | 440kB 10.2MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 450kB 10.2MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 460kB 10.2MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 471kB 10.2MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 481kB 10.2MB/s eta 0:00:01\r\u001b[K     |█████████                       | 491kB 10.2MB/s eta 0:00:01\r\u001b[K     |█████████                       | 501kB 10.2MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 512kB 10.2MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 522kB 10.2MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 532kB 10.2MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 542kB 10.2MB/s eta 0:00:01\r\u001b[K     |██████████                      | 552kB 10.2MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 563kB 10.2MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 573kB 10.2MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 583kB 10.2MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 593kB 10.2MB/s eta 0:00:01\r\u001b[K     |███████████                     | 604kB 10.2MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 614kB 10.2MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 624kB 10.2MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 634kB 10.2MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 645kB 10.2MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 655kB 10.2MB/s eta 0:00:01\r\u001b[K     |████████████                    | 665kB 10.2MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 675kB 10.2MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 686kB 10.2MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 696kB 10.2MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 706kB 10.2MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 716kB 10.2MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 727kB 10.2MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 737kB 10.2MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 747kB 10.2MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 757kB 10.2MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 768kB 10.2MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 778kB 10.2MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 788kB 10.2MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 798kB 10.2MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 808kB 10.2MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 819kB 10.2MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 829kB 10.2MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 839kB 10.2MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 849kB 10.2MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 860kB 10.2MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 870kB 10.2MB/s eta 0:00:01\r\u001b[K     |████████████████                | 880kB 10.2MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 890kB 10.2MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 901kB 10.2MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 911kB 10.2MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 921kB 10.2MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 931kB 10.2MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 942kB 10.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 952kB 10.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 962kB 10.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 972kB 10.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 983kB 10.2MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 993kB 10.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 1.0MB 10.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 1.0MB 10.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 1.0MB 10.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 1.0MB 10.2MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.0MB 10.2MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.1MB 10.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 1.1MB 10.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 1.1MB 10.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 1.1MB 10.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 1.1MB 10.2MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.1MB 10.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 1.1MB 10.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 1.1MB 10.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 1.1MB 10.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 1.1MB 10.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.2MB 10.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 1.2MB 10.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.2MB 10.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 1.2MB 10.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 1.2MB 10.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 1.2MB 10.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.2MB 10.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.2MB 10.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.2MB 10.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.2MB 10.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.3MB 10.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.3MB 10.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.3MB 10.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.3MB 10.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.3MB 10.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.3MB 10.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.3MB 10.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.3MB 10.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.3MB 10.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.4MB 10.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.4MB 10.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.4MB 10.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.4MB 10.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.4MB 10.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.4MB 10.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 1.4MB 10.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 1.4MB 10.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.4MB 10.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.4MB 10.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.5MB 10.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.5MB 10.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.5MB 10.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.5MB 10.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.5MB 10.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.5MB 10.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.5MB 10.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.5MB 10.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.5MB 10.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.5MB 10.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.6MB 10.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.6MB 10.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.6MB 10.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.6MB 10.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.6MB 10.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.6MB 10.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.6MB 10.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.6MB 10.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.6MB 10.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.6MB 10.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.7MB 10.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.7MB 10.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.7MB 10.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.7MB 10.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.7MB 10.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.7MB 10.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.7MB 10.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.7MB 10.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.7MB 10.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.8MB 10.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.8MB 10.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.8MB 10.2MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n","Collecting tokenizers==0.9.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n","\u001b[K     |████████████████████████████████| 2.9MB 48.3MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.3.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 50.2MB/s \n","\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=23fddbe538ca8c07d9eeb1407207c54fabef2a06d25ded4ac6793e46cefe1374\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sacremoses, transformers\n","Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.2.1\n","Collecting SentencePiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/67/e42bd1181472c95c8cda79305df848264f2a7f62740995a46945d9797b67/sentencepiece-0.1.95-cp36-cp36m-manylinux2014_x86_64.whl (1.2MB)\n","\u001b[K     |████████████████████████████████| 1.2MB 7.8MB/s \n","\u001b[?25hInstalling collected packages: SentencePiece\n","Successfully installed SentencePiece-0.1.95\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_dtxFCVzHkVg"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","os.chdir('/content/drive/My Drive/Colab Notebooks/4/')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pzM1_ykHaFur"},"source":["# Importing required libraries\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n","\n","# Importing the T5 modules from huggingface/transformers\n","from transformers import T5Tokenizer, T5ForConditionalGeneration\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KvPxXdKJguYB","outputId":"2758f793-543a-4190-dab2-da8ab62f2e3e"},"source":["# Checking out the GPU we have access to. This is output is from the google colab version. \n","!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Fri Jan 15 05:46:44 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.27.04    Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   58C    P8    10W /  70W |      0MiB / 15079MiB |      0%      Default |\n","|                               |                      |                 ERR! |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NLxxwd1scQNv"},"source":["# # Setting up the device for GPU usage\n","from torch import cuda\n","device = 'cuda' if cuda.is_available() else 'cpu'\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M4a9jQwESddT"},"source":["<a id='section02'></a>\n","### Preparing the Dataset for data processing: Class\n","\n","We will start with creation of Dataset class - This defines how the text is pre-processed before sending it to the neural network. This dataset will be used the the Dataloader method that will feed  the data in batches to the neural network for suitable training and processing. \n","The Dataloader and Dataset will be used in the main process later.\n","Dataset and Dataloader are constructs of the PyTorch library for defining and controlling the data pre-processing and its passage to neural network. For further reading into Dataset and Dataloader read the [docs at PyTorch](https://pytorch.org/docs/stable/data.html)\n","\n","#### *CustomDataset* Dataset Class\n","- This class is defined to accept the Dataframe as input and generate tokenized output that is used by the **T5** model for training. \n","- We are using the **T5** tokenizer to tokenize the data in the `text` and `ctext` column of the dataframe. \n","- The tokenizer uses the ` batch_encode_plus` method to perform tokenization and generate the necessary outputs, namely: `source_id`, `source_mask` from the actual text and `target_id` and `target_mask` from the summary text.\n","- To read further into the tokenizer, [refer to this document](https://huggingface.co/transformers/model_doc/t5.html#t5tokenizer)\n","- The *CustomDataset* class is used to create 2 datasets, for training and for validation.\n","- *Training Dataset* is used to fine tune the model: **80% of the original data**\n","- *Validation Dataset* is used to evaluate the performance of the model. The model has not seen this data during training. \n","\n","#### Dataloader: Called in the main process\n","- Dataloader is used to for creating training and validation dataloader that load data to the neural network in a defined manner. This is needed because all the data from the dataset cannot be loaded to the memory at once, hence the amount of data loaded to the memory and then passed to the neural network needs to be controlled.\n","- This control is achieved using the parameters such as `batch_size` and `max_len`.\n","- Training and Validation dataloaders are used in the training and validation part of the flow respectively"]},{"cell_type":"code","metadata":{"id":"932p8NhxeNw4"},"source":["# Creating a custom dataset for reading the dataframe and loading it into the dataloader to pass it to the neural network at a later stage for finetuning the model and to prepare it for predictions\n","\n","class CustomDataset(Dataset):\n","\n","    def __init__(self, dataframe, tokenizer, source_len, summ_len):\n","        self.tokenizer = tokenizer\n","        self.data = dataframe\n","        self.source_len = source_len\n","        self.summ_len = summ_len\n","        self.text = self.data.text\n","        self.ctext = self.data.ctext\n","\n","    def __len__(self):\n","        return len(self.text)\n","\n","    def __getitem__(self, index):\n","        ctext = str(self.ctext[index])\n","        ctext = ' '.join(ctext.split())\n","\n","        text = str(self.text[index])\n","        text = ' '.join(text.split())\n","\n","        source = self.tokenizer.batch_encode_plus([ctext], max_length= self.source_len, pad_to_max_length=True,return_tensors='pt')\n","        target = self.tokenizer.batch_encode_plus([text], max_length= self.summ_len, pad_to_max_length=True,return_tensors='pt')\n","\n","        source_ids = source['input_ids'].squeeze()\n","        source_mask = source['attention_mask'].squeeze()\n","        target_ids = target['input_ids'].squeeze()\n","        target_mask = target['attention_mask'].squeeze()\n","\n","        return {\n","            'source_ids': source_ids.to(dtype=torch.long), \n","            'source_mask': source_mask.to(dtype=torch.long), \n","            'target_ids': target_ids.to(dtype=torch.long),\n","            'target_ids_y': target_ids.to(dtype=torch.long)\n","        }"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FG4OyuoJSddU"},"source":["<a id='section03'></a>\n","### Fine Tuning the Model: Function\n","\n","Here we define a training function that trains the model on the training dataset created above, specified number of times (EPOCH), An epoch defines how many times the complete data will be passed through the network. \n","\n","This function is called in the main process.\n","\n","Following events happen in this function to fine tune the neural network:\n","- The epoch, tokenizer, model, device details, testing_ dataloader and optimizer are passed to the `train ()` when its called from the `main()`\n","- The dataloader passes data to the model based on the batch size.\n","- `language_model_labels` are calculated from the `target_ids` also, `source_id` and `attention_mask` are extracted.\n","- The model outputs first element gives the loss for the forward pass. \n","- Loss value is used to optimize the weights of the neurons in the network.\n","- After every 500 steps the loss value is printed in the console."]},{"cell_type":"code","metadata":{"id":"SaPAR7TWmxoM"},"source":["# Creating the training function. This will be called in the main process. It is run depending on the epoch value.\n","# The model is put into train mode and then we enumerate over the training loader and passed to the defined network \n","\n","def train(epoch, tokenizer, model, device, loader, optimizer):\n","    model.train()\n","    for _,data in enumerate(loader, 0):\n","        y = data['target_ids'].to(device, dtype = torch.long)\n","        y_ids = y[:, :-1].contiguous()\n","        lm_labels = y[:, 1:].clone().detach()\n","        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n","        ids = data['source_ids'].to(device, dtype = torch.long)\n","        mask = data['source_mask'].to(device, dtype = torch.long)\n","\n","        outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, labels=lm_labels)\n","        loss = outputs[0]\n","        \n","        if _%500==0:\n","            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n","        \n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KOPBDJ-0SddU"},"source":["<a id='section04'></a>\n","### Validating the Model Performance: Function\n","\n","During the validation stage we pass the unseen data(Testing Dataset), trained model, tokenizer and device details to the function to perform the validation run. This step generates new summary for dataset that it has not seen during the training session. \n","\n","This function is called in the main process.\n","\n","This unseen data is the 20% of `news_summary.csv` which was seperated during the Dataset creation stage. \n","During the validation stage the weights of the model are not updated. We use the generate method for generating new text for the summary. \n","\n","It depends on the `Beam-Search coding` method developed for sequence generation for models with LM head. \n","\n","The generated text and original summary are decoded from tokens to text and returned."]},{"cell_type":"code","metadata":{"id":"j9TNdHlQ0CLz"},"source":["def validate(epoch, tokenizer, model, device, loader):\n","    model.eval()\n","    predictions = []\n","    actuals = []\n","    with torch.no_grad():\n","        for _, data in enumerate(loader, 0):\n","            y = data['target_ids'].to(device, dtype = torch.long)\n","            ids = data['source_ids'].to(device, dtype = torch.long)\n","            mask = data['source_mask'].to(device, dtype = torch.long)\n","\n","            generated_ids = model.generate(\n","                input_ids = ids,\n","                attention_mask = mask, \n","                max_length=150, \n","                num_beams=2,\n","                repetition_penalty=2.5, \n","                length_penalty=1.0, \n","                early_stopping=True\n","                )\n","            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n","            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n","            if _%100==0:\n","                print(f'Completed {_}')\n","\n","            predictions.extend(preds)\n","            actuals.extend(target)\n","    return predictions, actuals"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aHvWP_nPSddU"},"source":["<a id='section05'></a>\n","### The Main Process\n","\n","\n","We first define configuration parameters that will be used across the tutorial such as `batch_size`, `epoch`, `learning_rate` etc., and also define seed values so that the experiment and results can be reproduced.\n","\n","\n","<a id='section502'></a>\n","#### Importing and Pre-Processing the domain data\n","\n","*Assuming that the `news_summary.csv` is already uploaded into the respective folder*\n","\n","* The file is imported as a dataframe and give it the headers as per the documentation.\n","* Cleaning the file to remove the unwanted columns.\n","* A new string is added to the main article column `summarize: ` prior to the actual article. This is done because **T5** had similar formatting for the summarization dataset. \n","* The final Dataframe will be something like this:\n","\n","|text|ctext|\n","|--|--|\n","|summary-1|summarize: article 1|\n","|summary-2|summarize: article 2|\n","|summary-3|summarize: article 3|\n","\n","* Top 5 rows of the dataframe are printed on the console.\n","\n","<a id='section503'></a>\n","#### Creation of Dataset and Dataloader\n","\n","* The updated dataframe is divided into 80-20 ratio for test and validation. \n","* Both the data-frames are passed to the `CustomerDataset` class for tokenization of the articles and their summaries.\n","* The tokenization is done using the length parameters passed to the class.\n","* Train and Validation parameters are defined and passed to the `pytorch Dataloader contstruct` to create `train` and `validation` data loaders.\n","* These dataloaders will be passed to `train()` and `validate()` respectively for training and validation action.\n","* The shape of datasets is printed in the console.\n","\n","\n","<a id='section504'></a>\n","#### Neural Network and Optimizer\n","\n","* In this stage we define the model and optimizer that will be used for training and to update the weights of the network. \n","* We are using the `t5-base` transformer model for our project. You can read about the `T5 model` and its features above. \n","* We use the `T5ForConditionalGeneration.from_pretrained(\"t5-base\")` commad to define our model. The `T5ForConditionalGeneration` adds a Language Model head to our `T5 model`. The Language Model head allows us to generate text based on the training of `T5 model`.\n","* We are using the `Adam` optimizer for our project. This  is something that can be changed to see how different optimizer perform with different learning rates. \n","\n","\n","\n","<a id='section505'></a>\n","#### Training Model\n","\n","* Now we call the `train()` with all the necessary parameters.\n","* Loss at every 500th step is printed on the console.\n","\n","\n","\n","<a id='section506'></a>\n","#### Validation and generation of Summary\n","\n","* After the training is completed, the validation step is initiated.\n","* As defined in the validation function, the model weights are not updated. We use the fine tuned model to generate new summaries based on the article text.\n","* An output is printed on the console giving a count of how many steps are complete after every 100th step. \n","* The original summary and generated summary are converted into a list and returned to the main function. \n","* Both the lists are used to create the final dataframe with 2 columns **Generated Summary** and **Actual Summary**\n","* The dataframe is saved as a csv file in the local drive.\n","* A qualitative analysis can be done with the Dataframe. "]},{"cell_type":"code","metadata":{"id":"VkiTvE0PX_7w"},"source":["# Defining some key variables that will be used later on in the training  \n","TRAIN_BATCH_SIZE = 2    # input batch size for training (default: 64)\n","VALID_BATCH_SIZE = 2    # input batch size for testing (default: 1000)\n","TRAIN_EPOCHS = 2        # number of epochs to train (default: 10)\n","VAL_EPOCHS = 1 \n","LEARNING_RATE = 1e-4    # learning rate (default: 0.01)\n","SEED = 42               # random seed (default: 42)\n","MAX_LEN = 512\n","SUMMARY_LEN = 150 "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o22q8Wq0YblS","colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["b0b20bf692004a19b33a2971502eb448","dcb6aa15ba4b4718b7f60728d5a83ba1","9679bf8840b247a984af62b6c64caa15","949edc33c0b548d0b16ffe7122c46721","d7986cb8f5d34c7ea66d1a45a22d9ee8","fa5159bd32cd42dcac3bb73e94ff6186","843f88f818494abaa40772f3e3ee3dfb","569c847de00b4a89a65ceda90782249f"]},"outputId":"1eba4c9b-945d-46e8-e9f8-d2826bb56551"},"source":["# Set random seeds and deterministic pytorch for reproducibility\n","torch.manual_seed(SEED) # pytorch random seed\n","np.random.seed(SEED) # numpy random seed\n","torch.backends.cudnn.deterministic = True\n","\n","# tokenzier for encoding the text\n","tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b0b20bf692004a19b33a2971502eb448","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=791656.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pKttXFNHWH5_","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3ed146d5-b376-4654-ad6c-2e24fbb3117a"},"source":["df = pd.read_csv('./news_summary.csv', encoding='latin-1')\n","df = df[['text','ctext']]\n","df.ctext = 'summarize: ' + df.ctext\n","print(df.head())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["                                                text                                              ctext\n","0  The Administration of Union Territory Daman an...  summarize: The Daman and Diu administration on...\n","1  Malaika Arora slammed an Instagram user who tr...  summarize: From her special numbers to TV?appe...\n","2  The Indira Gandhi Institute of Medical Science...  summarize: The Indira Gandhi Institute of Medi...\n","3  Lashkar-e-Taiba's Kashmir commander Abu Dujana...  summarize: Lashkar-e-Taiba's Kashmir commander...\n","4  Hotels in Maharashtra will train their staff t...  summarize: Hotels in Mumbai and other Indian c...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xKD_9SuAhJzC","outputId":"67e58db5-6dee-463a-c271-921d969ce7b0"},"source":["# Creation of Dataset and Dataloader\n","# Defining the train size. So 80% of the data will be used for training and the rest will be used for validation. \n","train_size = 0.8\n","train_dataset=df.sample(frac=train_size,random_state = SEED)\n","val_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n","train_dataset = train_dataset.reset_index(drop=True)\n","\n","print(\"FULL Dataset: {}\".format(df.shape))\n","print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n","print(\"TEST Dataset: {}\".format(val_dataset.shape))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["FULL Dataset: (4514, 2)\n","TRAIN Dataset: (3611, 2)\n","TEST Dataset: (903, 2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fsBMybWXhcLV"},"source":["# Creating the Training and Validation dataset for further creation of Dataloader\n","training_set = CustomDataset(train_dataset, tokenizer, MAX_LEN, SUMMARY_LEN)\n","val_set = CustomDataset(val_dataset, tokenizer, MAX_LEN, SUMMARY_LEN)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L-KvtdQ-hvFp"},"source":["# Defining the parameters for creation of dataloaders\n","train_params = {\n","    'batch_size': TRAIN_BATCH_SIZE,\n","    'shuffle': True,\n","    'num_workers': 0\n","     }\n","\n","val_params = {\n","    'batch_size': VALID_BATCH_SIZE,\n","    'shuffle': False,\n","    'num_workers': 0\n","    }\n","\n","# Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n","training_loader = DataLoader(training_set, **train_params)\n","val_loader = DataLoader(val_set, **val_params)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HzuKoZgVh_Nj"},"source":["# Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary. \n","# Further this model is sent to device (GPU/TPU) for using the hardware.\n","model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n","model = model.to(device)\n","\n","# Defining the optimizer that will be used to tune the weights of the network in the training session. \n","optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5SKFUho3iQ7J","outputId":"93e8c44a-4e57-4e03-cbe2-13253f8e690e"},"source":["# Training loop\n","print('Initiating Fine-Tuning for the model on our dataset')\n","\n","for epoch in range(TRAIN_EPOCHS):\n","    train(epoch, tokenizer, model, device, training_loader, optimizer)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"],"name":"stderr"},{"output_type":"stream","text":["Initiating Fine-Tuning for the model on our dataset\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2143: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 0, Loss:  5.02570915222168\n","Epoch: 0, Loss:  1.131319522857666\n","Epoch: 0, Loss:  1.7562652826309204\n","Epoch: 0, Loss:  1.5954517126083374\n","Epoch: 1, Loss:  2.283595323562622\n","Epoch: 1, Loss:  1.0396584272384644\n","Epoch: 1, Loss:  1.3172086477279663\n","Epoch: 1, Loss:  1.997849702835083\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3dUuL7ENn-ee","outputId":"9ec7770f-d3f4-4a59-81c7-41a4f37972aa"},"source":["# Validation loop and saving the resulting file with predictions and acutals in a dataframe.\n","# Saving the dataframe as predictions.csv\n","print('Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe')\n","for epoch in range(VAL_EPOCHS):\n","    predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n","    final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n","    final_df.to_csv('./predictions.csv')\n","    print('Output Files generated for review')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2143: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Completed 0\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2143: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Completed 100\n","Completed 200\n","Completed 300\n","Completed 400\n","Output Files generated for review\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NnUiDFewSddU"},"source":["<a id='section06'></a>\n","### Examples of the Summary Generated from the model\n","\n","##### Example 1\n","\n","**[Original Text]**\n","New Delhi, Apr 25 (PTI) Union minister Vijay Goel today batted for the unification of the three municipal corporations in the national capital saying a discussion over the issue was pertinent. The BJP leader, who was confident of a good show by his party in the MCD polls, the results of which will be declared tomorrow, said the civic bodies needed to be \"revamped\" in order to deliver the services to the people more effectively. The first thing needed was a discussion on the unification of the three municipal corporations and there should also be an end to the practice of sending Delhi government officials to serve in the civic bodies, said the Union Minister of State (Independent Charge) for Youth Affairs and Sports. \"Barring one, the two other civic bodies have been incurring losses. It would be more fruitful and efficient if all the three were merged,\" he said, referring to the north, south and east Delhi municipal corporations. The erstwhile Municipal Corporation of Delhi (MCD) was trifurcated into NDMC, SDMC and EDMC by the then Sheila Dikshit-led Delhi government in 2012. Goel predicted a \"thumping\" victory for the BJP in the MCD polls. He said the newly-elected BJP councillors will be trained on the functioning of the civic bodies and dealing with the bureaucracy. \n","\n","\n","**[Original Summary]**\n","Union Minister Vijay Goel has favoured unification of three MCDs ? North, South and East ? in order to deliver the services more effectively. \"Barring one, the two other civic bodies have been incurring losses. It would be more fruitful and efficient if all the three were merged,\" he said. MCD was trifurcated into EDMC, NDMC and SDMC in 2012.\n","\n","**[Generated Summary]**\n","BJP leader Vijay Goel on Saturday batted for the unification of three municipal corporations in the national capital saying a discussion over this was pertinent. \"Barring one, two other civic bodies have been incurring losses,\" said Goels. The erstwhile Municipal Corporations of Delhi (MCD) were trifurcated into NDMC and SDMC by the then Sheilha Dikshi-led government in 2012. Notably, the MCD poll results will be declared tomorrow."]},{"cell_type":"markdown","metadata":{"id":"aVAfTCbmSddU"},"source":["##### Example 2\n","\n","**[Original Text]** \n","After much wait, the first UDAN flight took off from Shimla today after being flagged off by Prime Minister Narendra Modi.The flight will be operated by Alliance Air, the regional arm of Air India. PM Narendra Modi handed over boarding passes to some of passengers travelling via the first UDAN flight at the Shimla airport.Tomorrow PM @narendramodi will flag off the first UDAN flight under the Regional Connectivity Scheme, on Shimla-Delhi sector.Air India yesterday opened bookings for the first launch flight from Shimla to Delhi with all inclusive fares starting at Rs2,036.THE GREAT 'UDAN'The UDAN (Ude Desh ka Aam Naagrik) scheme seeks to make flying more affordable for the common people, holding a plan to connect over 45 unserved and under-served airports.Under UDAN, 50 per cent of the seats on each flight would have a cap of Rs 2,500 per seat/hour. The government has also extended subsidy in the form of viability gap funding to the operators flying on these routes.The scheme was launched to \"make air travel accessible to citizens in regionally important cities,\" and has been described as \"a first-of-its-kind scheme globally to stimulate regional connectivity through a market-based mechanism.\" Report have it the first flight today will not be flying at full capacity on its 70-seater ATR airplane because of payload restrictions related to the short Shimla airfield.|| Read more ||Udan scheme: Now you can fly to these 43 cities, see the full list hereUDAN scheme to fly hour-long flights capped at Rs 2,500 to smaller cities \n","\n","\n","**[Original Summary]** \n","PM Narendra Modi on Thursday launched Ude Desh ka Aam Nagrik (UDAN) scheme for regional flight connectivity by flagging off the inaugural flight from Shimla to Delhi. Under UDAN, government will connect small towns by air with 50% plane seats' fare capped at?2,500 for a one-hour journey of 500 kilometres. UDAN will connect over 45 unserved and under-served airports.\n","\n","**[Generated Summary]** \n","UDAN (Ude Desh Ka Aam Naagrik) scheme, launched to make air travel accessible in regionally important cities under the Regional Connectivity Scheme, took off from Shimla on Tuesday. The first flight will be operated by Alliance Air, which is the regional arm of India's Air India. Under the scheme, 50% seats would have?2,500 per seat/hour and 50% of the seats would have capped at this rate. It was also extended subsidy in form-based funding for operators flying these routes as well."]},{"cell_type":"markdown","metadata":{"id":"vMnvEcFUSddU"},"source":["##### Example 3\n","\n","**[Original Text]**\n","New Delhi, Apr 25 (PTI) The Income Tax department has issued a Rs 24,646 crore tax demand notice to Sahara Groups Aamby Valley Limited (AVL) after conducting a special audit of the company. The department, as part of a special investigation and audit into the account books of AVL, found that an income of over Rs 48,000 crore for a particular assessment year was allegedly not reflected in the record books of the firm and hence it raised a fresh tax demand and penalty amount on it. A Sahara Group spokesperson confirmed the development to PTI. \"Yes, the Income Tax Department has raised Rs 48,085.79 crores to the income of the Aamby Valley Limited with a total demand of income tax of Rs 24,646.96 crores on the Aamby Valley Limited,\" the spokesperson said in a brief statement. Officials said the notice was issued by the taxman in January this year after the special audit of AVLs income for the Assessment Year 2012-13 found that the parent firm had allegedly floated a clutch of Special Purpose Vehicles whose incomes were later accounted on the account of AVL as they were merged with the former in due course of time. The AVL, in its income return filed for AY 2012-13, had reflected a loss of few crores but the special I-T audit brought up the added income, a senior official said. The Supreme Court, last week, had asked the Bombay High Courts official liquidator to sell the Rs 34,000 crore worth of properties of Aamby Valley owned by the Sahara Group and directed its chief Subrata Roy to personally appear before it on April 28.  \n","\n","\n","**[Original Summary]**\n","The Income Tax Department has issued a ?24,646 crore tax demand notice to Sahara Group's Aamby Valley Limited. The department's audit found that an income of over ?48,000 crore for the assessment year 2012-13 was not reflected in the record books of the firm. A week ago, the SC ordered Bombay HC to auction Sahara's Aamby Valley worth ?34,000 crore.\n","\n","**[Generated Summary]**\n","the Income Tax department has issued a?24,646 crore tax demand notice to Sahara Groups Aamby Valley Limited (AVL) after conducting an audit of the company. The notice was issued in January this year after the special audit found that the parent firm had floated Special Purpose Vehicle income for the Assessment Year 2012-13 and later accounted on its account as they were merged with the former. \"Yes...the Income Tax Department raised Rs48,085.79 crores to the income,\" he added earlier said at the notice."]},{"cell_type":"markdown","metadata":{"id":"xBDfds3osWTy"},"source":["#Reference\n","Fine Tuneing T5 for Summary Generation by Abhishek Kumar Mishra (https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_summarization_wandb.ipynb)\n","\n","Documentation on T5 by HuggingFace (https://huggingface.co/transformers/model_doc/t5.html)"]}]}