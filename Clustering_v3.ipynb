{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled1.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMNIM8Ac7zx9d17hcfCzCgb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"7swrfINLLfBv"},"source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on Mon Aug 19 11:04:54 2019\n","\n","@author: isswan\n","\"\"\"\n","# In this workshop we perform document clustering using sklearn\n","\n","import logging\n","logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n","\n","# Remember, in actual use of document clustering, the documents DON'T come with labeled classes.\n","# It's unsupervised learning.\n","import numpy as np\n","import pandas as pd\n","\n","\n","news=pd.read_table('r8-train-all-terms.txt',header=None,names = [\"Class\", \"Text\"])\n","news.groupby('Class').size()\n","\n","subnews=news[(news.Class==\"interest\")| (news.Class=='crude')|(news.Class=='money-fx') ]\n","subnews.head()\n","subnews.groupby('Class').size()\n","\n","\n","###################################### Preprocessing\n","\n","import nltk\n","from nltk.corpus import stopwords\n","mystopwords=stopwords.words(\"english\") + ['one', 'become', 'get', 'make', 'take']\n","\n","WNlemma = nltk.WordNetLemmatizer()\n","\n","def pre_process(text):\n","    tokens = nltk.word_tokenize(text)\n","    tokens=[ WNlemma.lemmatize(t.lower()) for t in tokens]\n","    tokens=[ t for t in tokens if t not in mystopwords]\n","    tokens = [ t for t in tokens if len(t) >= 3 ]\n","    text_after_process=\" \".join(tokens)\n","    return(text_after_process)\n","\n","# Apply preprocessing to every document in the training set.\n","text = subnews['Text']\n","toks = text.apply(pre_process)\n","\n","####################################### TDM\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.preprocessing import Normalizer\n","from sklearn.decomposition import TruncatedSVD\n","from sklearn.pipeline import make_pipeline\n","\n","# Create tfidf matrix\n","vectorizer = TfidfVectorizer(max_df=0.7, max_features=2500,\n","                             min_df=3, stop_words=mystopwords,\n","                             use_idf=True)\n","X = vectorizer.fit_transform(toks)\n","X.shape\n","\n","\n","####################################### Apply KMeans for clustering\n","from sklearn.cluster import KMeans\n","from sklearn import metrics\n","\n","\n","#‘k-means++’ : selects initial cluster centers for k-mean clustering in a smart way to speed up convergence.\n","#Maximum number of iterations of the k-means algorithm for a single run.\n","\n","km3 = KMeans(n_clusters=3, init='k-means++', max_iter=2000,random_state=5)\n","km3.fit(X)\n","\n","# Evaluate the 3 clusters \n","# Coefficient: more similar within clusters, more distant between clusters\n","# The higher the better (-1 to 1)\n","\n","print(\"Coefficient for 3 clusters: %0.3f\"\n","      % metrics.silhouette_score(X, km3.labels_))\n","\n","####################################### How many dos in each cluster\n","labels, counts = np.unique(km3.labels_[km3.labels_>=0], return_counts=True)\n","print (labels)\n","print (counts)\n","\n","subnews.groupby('Class').size()\n","\n","######################################### What are the clusters about\n","# note: Clustering only gives you index of cluster rather than the meaning of cluster\n","# need to review the docs in each cluster and summarize \n","# We still need to see the more representative words for each cluster to understand them.\n","\n","def print_terms(cm, num):\n","    original_space_centroids = cm.cluster_centers_\n","    order_centroids = original_space_centroids.argsort()[:, ::-1]\n","    terms = vectorizer.get_feature_names()\n","    for i in range(num):\n","        print(\"Cluster %d:\" % i, end='')\n","        for ind in order_centroids[i, :10]:\n","            print(' %s' % terms[ind], end='')\n","        print()\n","\n","\n","print_terms(km3, 3)\n","\n","\n","# Let's assign the cluster label to the categories \n","# Note: the order of clusters may change for differnt runs \n","\n","dict = {0: 'interest', 1: 'crude', 2: 'money-fx'}\n","print(dict)\n","print(counts)\n","subnews.groupby('Class').size()\n","\n","######################################### Evaluation: Assume we have the annotations \n","#get the category for each document\n","cluster_labels = [ dict[c] for c in km3.labels_]\n","correct_labels = subnews['Class']\n","print(metrics.confusion_matrix(cluster_labels, correct_labels))\n","print(np.mean(cluster_labels == correct_labels) )\n","print(metrics.classification_report(cluster_labels, correct_labels))\n","\n","########################################\n","####################################### Use SVD to reduce dimensions\n","svd = TruncatedSVD(300)\n","normalizer = Normalizer(copy=False)\n","lsa = make_pipeline(svd, normalizer)\n","X_lsa = lsa.fit_transform(X)\n","\n","\n","\n","#set to False to perform inplace row normalization\n","# Check how much \"variance is explained\" (information is kept)\n","#explained_variance = svd.explained_variance_ratio_.sum()\n","#print(\"Explained variance of the SVD step: {}%\".format(int(explained_variance * 100)))\n","\n","####################################### Apply KMeans for clustering\n","from sklearn.cluster import KMeans\n","\n","km3 = KMeans(n_clusters=3, init='k-means++', max_iter=1000, n_init=1)\n","km3.fit(X_lsa)\n","\n","from sklearn import metrics\n","\n","\n","print(\"Coefficient for 3 clusters: %0.3f\" % metrics.silhouette_score(X_lsa, km3.labels_))\n","\n","\n","labels, counts = np.unique(km3.labels_[km3.labels_>=0], return_counts=True)\n","print (labels)\n","print (counts)\n","\n","subnews.groupby('Class').size()\n","\n","def print_SVD_terms(cm, num):\n","    original_space_centroids = svd.inverse_transform(cm.cluster_centers_)\n","    order_centroids = original_space_centroids.argsort()[:, ::-1]\n","    terms = vectorizer.get_feature_names()\n","    for i in range(num):\n","        print(\"Cluster %d:\" % i, end='')\n","        for ind in order_centroids[i, :10]:\n","            print(' %s' % terms[ind], end='')\n","        print()\n","\n","print_SVD_terms(km3, 3)\n","\n","# Let's assign the cluster label to the categories \n","# Note: the order of clusters may change for differnt runs \n","dict = {0: 'money-fx', 1: 'crude', 2: 'interest'}\n","cluster_labels = [ dict[c] for c in km3.labels_]\n","\n","####Let's check out the confusion matrix of clustering results\n","##### Remember, in actual use of document clustering, the documents DON'T come with labeled classes.\n","##### So nomally we can not access the confusion matrix unless we label some data manually \n","import numpy as np\n","print(metrics.confusion_matrix(cluster_labels, correct_labels))\n","print(np.mean(cluster_labels == correct_labels) )\n","print(metrics.classification_report(cluster_labels, correct_labels))\n"],"execution_count":null,"outputs":[]}]}