{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"TPML_3_Transformer for summarizing_Li_Yuejun.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"tf","language":"python","name":"tf"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"}},"cells":[{"cell_type":"code","metadata":{"id":"Zbxhyl_zFlWL"},"source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","import time\n","import re\n","import pickle\n","import datetime\n","import io\n","import unicodedata\n","a = datetime.datetime.now()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BVLu3cDBREt4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630938798250,"user_tz":-480,"elapsed":21997,"user":{"displayName":"Yuejun Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgS0aPSLsqcw7xL7MdcefrmuNtIFiV2gDoJusqnqg=s64","userId":"03773758233632074141"}},"outputId":"d67e7574-4c34-4016-8c43-3d26a8b7e30b"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"WpiG3Tbcggxu"},"source":["import os\n","os.chdir('/content/drive/My Drive/Colab Notebooks/4/')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yH5cg5pSIHaZ"},"source":["### Loading Data"]},{"cell_type":"code","metadata":{"id":"K_AjGkWXITKA","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1630938817450,"user_tz":-480,"elapsed":6042,"user":{"displayName":"Yuejun Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgS0aPSLsqcw7xL7MdcefrmuNtIFiV2gDoJusqnqg=s64","userId":"03773758233632074141"}},"outputId":"0e2afbc3-990a-4c28-b9d3-b04d4023cb2f"},"source":["news = pd.read_excel(\"news.xlsx\")\n","news.drop(['Source ', 'Time ', 'Publish Date'], axis=1, inplace=True)\n","news.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Headline</th>\n","      <th>Short</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>4 ex-bank officials booked for cheating bank o...</td>\n","      <td>The CBI on Saturday booked four former officia...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Supreme Court to go paperless in 6 months: CJI</td>\n","      <td>Chief Justice JS Khehar has said the Supreme C...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>At least 3 killed, 30 injured in blast in Sylh...</td>\n","      <td>At least three people were killed, including a...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Why has Reliance been barred from trading in f...</td>\n","      <td>Mukesh Ambani-led Reliance Industries (RIL) wa...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Was stopped from entering my own studio at Tim...</td>\n","      <td>TV news anchor Arnab Goswami has said he was t...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            Headline                                              Short\n","0  4 ex-bank officials booked for cheating bank o...  The CBI on Saturday booked four former officia...\n","1     Supreme Court to go paperless in 6 months: CJI  Chief Justice JS Khehar has said the Supreme C...\n","2  At least 3 killed, 30 injured in blast in Sylh...  At least three people were killed, including a...\n","3  Why has Reliance been barred from trading in f...  Mukesh Ambani-led Reliance Industries (RIL) wa...\n","4  Was stopped from entering my own studio at Tim...  TV news anchor Arnab Goswami has said he was t..."]},"metadata":{},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"vR2hg9themaN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630754011548,"user_tz":-480,"elapsed":375,"user":{"displayName":"Yuejun Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgS0aPSLsqcw7xL7MdcefrmuNtIFiV2gDoJusqnqg=s64","userId":"03773758233632074141"}},"outputId":"5a686ff7-4c98-4526-f80f-1a98c26b4fdc"},"source":["news.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(55104, 2)"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"d4cEp3wmI2BX"},"source":["document = news['Short']   # input\n","summary = news['Headline']  # output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2z55AhpKIdK7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630765322411,"user_tz":-480,"elapsed":6,"user":{"displayName":"Yuejun Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgS0aPSLsqcw7xL7MdcefrmuNtIFiV2gDoJusqnqg=s64","userId":"03773758233632074141"}},"outputId":"0acd735f-6bf8-4492-bf9a-bb972859531e"},"source":["document[30], summary[30]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('According to the Guinness World Records, the most generations alive in a single family have been seven.  The difference between the oldest and the youngest person in the family was about 109 years, when Augusta Bunge&#39;s great-great-great-great grandson was born on January 21, 1989. The family belonged to the United States of America.',\n"," 'The most generations alive in a single family have been 7')"]},"metadata":{},"execution_count":28}]},{"cell_type":"markdown","metadata":{"id":"f8gKyq1gIq4r"},"source":["### Preprocessing"]},{"cell_type":"code","metadata":{"id":"TJ6LE4MrJjC_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630938846117,"user_tz":-480,"elapsed":292,"user":{"displayName":"Yuejun Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgS0aPSLsqcw7xL7MdcefrmuNtIFiV2gDoJusqnqg=s64","userId":"03773758233632074141"}},"outputId":"07a9527f-f992-408f-828d-68dc581e158d"},"source":["# for decoder sequence\n","summary = summary.apply(lambda x: '<go> ' + x + ' <stop>')\n","summary.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    <go> 4 ex-bank officials booked for cheating b...\n","1    <go> Supreme Court to go paperless in 6 months...\n","2    <go> At least 3 killed, 30 injured in blast in...\n","3    <go> Why has Reliance been barred from trading...\n","4    <go> Was stopped from entering my own studio a...\n","Name: Headline, dtype: object"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"95Zv7FIvKbTi"},"source":["#### Tokenizing the texts into integer tokens"]},{"cell_type":"code","metadata":{"id":"7TqbpEyPMRqa"},"source":["# since < and > from default tokens cannot be removed\n","filters = '!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n'\n","oov_token = '<unk>'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DWU9Xu7OKVab"},"source":["document_tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=oov_token)\n","summary_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=filters, oov_token=oov_token)\n","\n","document_tokenizer.fit_on_texts(document)  # customized tokenizer\n","summary_tokenizer.fit_on_texts(summary)  # customized tokenizer, different from input\n","\n","# Use text_to_sequence to obtain the sequence embedding\n","# which is simply a list of word embedding\n","inputs = document_tokenizer.texts_to_sequences(document)\n","targets = summary_tokenizer.texts_to_sequences(summary)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kVyErXAei5_b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630939442154,"user_tz":-480,"elapsed":273,"user":{"displayName":"Yuejun Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgS0aPSLsqcw7xL7MdcefrmuNtIFiV2gDoJusqnqg=s64","userId":"03773758233632074141"}},"outputId":"c485ecfc-7825-44a5-e45b-0b8562ceb4de"},"source":["summary_tokenizer.texts_to_sequences([\"This is a test\"])\n","summary_tokenizer.sequences_to_texts([[184, 22, 12, 71]])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['this is a test']"]},"metadata":{},"execution_count":43}]},{"cell_type":"code","metadata":{"id":"KoizyBvLKv8h","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630939446072,"user_tz":-480,"elapsed":7,"user":{"displayName":"Yuejun Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgS0aPSLsqcw7xL7MdcefrmuNtIFiV2gDoJusqnqg=s64","userId":"03773758233632074141"}},"outputId":"c0018475-3f56-46db-b67b-ae6922ad6252"},"source":["encoder_vocab_size = len(document_tokenizer.word_index) + 1\n","decoder_vocab_size = len(summary_tokenizer.word_index) + 1\n","\n","# vocab_size\n","encoder_vocab_size, decoder_vocab_size"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(76362, 29661)"]},"metadata":{},"execution_count":44}]},{"cell_type":"code","metadata":{"id":"btq1gH2rQzLX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630754155778,"user_tz":-480,"elapsed":5,"user":{"displayName":"Yuejun Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgS0aPSLsqcw7xL7MdcefrmuNtIFiV2gDoJusqnqg=s64","userId":"03773758233632074141"}},"outputId":"4a5aae7d-6afa-4a5c-a697-3e76085a2cc0"},"source":["len(document[0])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["379"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"mZden_q9_eZr"},"source":["#### Obtaining insights on lengths for defining maxlen"]},{"cell_type":"code","metadata":{"id":"ma4o2nGdK5Xb"},"source":["document_lengths = pd.Series([len(x) for x in document])  # \n","summary_lengths = pd.Series([len(x) for x in summary])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iXZlO99C-UXK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630754174553,"user_tz":-480,"elapsed":409,"user":{"displayName":"Yuejun Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgS0aPSLsqcw7xL7MdcefrmuNtIFiV2gDoJusqnqg=s64","userId":"03773758233632074141"}},"outputId":"f4cd94ba-9dd3-4b35-9899-3b8af6729684"},"source":["document_lengths.describe()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["count    55104.000000\n","mean       368.003049\n","std         26.235510\n","min        280.000000\n","25%        350.000000\n","50%        369.000000\n","75%        387.000000\n","max        469.000000\n","dtype: float64"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"ALMwKMx--ZF7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630754188738,"user_tz":-480,"elapsed":404,"user":{"displayName":"Yuejun Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgS0aPSLsqcw7xL7MdcefrmuNtIFiV2gDoJusqnqg=s64","userId":"03773758233632074141"}},"outputId":"db840580-1356-4b72-8d42-e7f33969db4b"},"source":["summary_lengths.describe()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["count    55104.000000\n","mean        63.620282\n","std          7.267463\n","min         20.000000\n","25%         59.000000\n","50%         63.000000\n","75%         69.000000\n","max         96.000000\n","dtype: float64"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"cVeMilXr-bpC"},"source":["# maxlen\n","# taking values > and round figured to 75th percentile\n","# at the same time not leaving high variance\n","encoder_maxlen = 400\n","decoder_maxlen = 75"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_SWap3YJBk-D"},"source":["#### Padding/Truncating sequences for identical sequence lengths\n","\n","This is actually a limitation in this case. All sentences longer than 400 words and its translated counterpart 75 are truncated to this limit. Input sentences that are shorter than 400 or target summary short then 75 are padded with excess zeros."]},{"cell_type":"code","metadata":{"id":"vEyUBeu7ACRt"},"source":["inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen=encoder_maxlen, padding='post', truncating='post')\n","targets = tf.keras.preprocessing.sequence.pad_sequences(targets, maxlen=decoder_maxlen, padding='post', truncating='post')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6HhvXU8vQzMH","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1630754236446,"user_tz":-480,"elapsed":10,"user":{"displayName":"Yuejun Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgS0aPSLsqcw7xL7MdcefrmuNtIFiV2gDoJusqnqg=s64","userId":"03773758233632074141"}},"outputId":"5f741374-2f60-437a-90c4-e2ed678c43c1"},"source":["summary[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'<go> 4 ex-bank officials booked for cheating bank of ₹209 crore <stop>'"]},"metadata":{},"execution_count":24}]},{"cell_type":"markdown","metadata":{"id":"6YJXy9IQOWnh"},"source":["Exmaple of padding the sentence shorter the maxlen."]},{"cell_type":"code","metadata":{"id":"wDtTX40dQzMM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630754242504,"user_tz":-480,"elapsed":613,"user":{"displayName":"Yuejun Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgS0aPSLsqcw7xL7MdcefrmuNtIFiV2gDoJusqnqg=s64","userId":"03773758233632074141"}},"outputId":"65fa251e-30c6-4ad2-a240-1e9cceb0b3da"},"source":["targets[0], len(targets[0])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([    2,    67,   129,   130,   731,   708,     8,  2303,   130,\n","            9, 13470,    43,     3,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0], dtype=int32), 75)"]},"metadata":{},"execution_count":25}]},{"cell_type":"markdown","metadata":{"id":"wIP0kIIcB8Rm"},"source":["### Creating dataset pipeline\n","This step is necessary as in tensorflow there are specifed data types."]},{"cell_type":"code","metadata":{"id":"LzO6l3-AB7hJ"},"source":["inputs = tf.cast(inputs, dtype=tf.int32)\n","targets = tf.cast(targets, dtype=tf.int32)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"slZ5f4P4DurS"},"source":["BUFFER_SIZE = 20000  # should be set at least equal to the size of the dataset\n","BATCH_SIZE = 64"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PvAHuz4nOtRy"},"source":["This step is to 'slice' the dataset into batches. Suppose the datasize consists of 100 text paragraphs, with BUFFER_SIZE = 20. \n","\n","There will be 5 iterations in each epoch. In each iteration, the gradients are re-calculated in the optimisation algorithm. \n","\n"]},{"cell_type":"code","metadata":{"id":"wI-fV7eABWN6"},"source":["dataset = tf.data.Dataset.from_tensor_slices((inputs, targets)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X0WvfNd7kWU9","executionInfo":{"status":"ok","timestamp":1630939478191,"user_tz":-480,"elapsed":274,"user":{"displayName":"Yuejun Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgS0aPSLsqcw7xL7MdcefrmuNtIFiV2gDoJusqnqg=s64","userId":"03773758233632074141"}},"outputId":"67f0336a-2f74-48ad-8ea5-203d13203207"},"source":["dataset"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<BatchDataset shapes: ((None, 400), (None, 75)), types: (tf.int32, tf.int32)>"]},"metadata":{},"execution_count":50}]},{"cell_type":"markdown","metadata":{"id":"isN1CpAXLfsl"},"source":["### Positional Encoding for adding notion of position among words as unlike RNN this is non-directional\n","\n","In a transformer, these positional encodings are passed to the encoder and decoder at the beginning."]},{"cell_type":"code","metadata":{"id":"Purv7oyhETDZ"},"source":["def get_angles(position, i, d_model):\n","    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n","    return position * angle_rates"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"40J2pc2NEXp5"},"source":["def positional_encoding(position, d_model):\n","    angle_rads = get_angles(\n","        np.arange(position)[:, np.newaxis],\n","        np.arange(d_model)[np.newaxis, :],\n","        d_model\n","    )\n","\n","    # apply sin to even indices in the array; 2i\n","    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n","\n","    # apply cos to odd indices in the array; 2i+1\n","    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n","\n","    pos_encoding = angle_rads[np.newaxis, ...]\n","\n","    return tf.cast(pos_encoding, dtype=tf.float32)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"24Pe01DMMWHc"},"source":["### Masking\n","\n","- Padding mask for masking \"pad\" sequences\n","- Lookahead mask for masking future words from contributing in prediction of current words in self attention\n","\n","The masking is applied to the decoding stage only."]},{"cell_type":"code","metadata":{"id":"hN1wVQAdMVYy"},"source":["def create_padding_mask(seq):\n","    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n","    return seq[:, tf.newaxis, tf.newaxis, :]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UmjAPLWuMREE"},"source":["def create_look_ahead_mask(size):\n","    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n","    return mask"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n8DqUBc4NFOy"},"source":["### Building the Model\n"]},{"cell_type":"markdown","metadata":{"id":"WfknVF7hNKf7"},"source":["#### Scaled Dot Product"]},{"cell_type":"code","metadata":{"id":"w_B6M9OBNBKB"},"source":["def scaled_dot_product_attention(q, k, v, mask):\n","    matmul_qk = tf.matmul(q, k, transpose_b=True)\n","\n","    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n","    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n","\n","    if mask is not None:\n","        scaled_attention_logits += (mask * -1e9)  \n","\n","    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n","\n","    output = tf.matmul(attention_weights, v)\n","    return output, attention_weights"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Rf7_a5uQOfJk"},"source":["#### Multi-Headed Attention"]},{"cell_type":"code","metadata":{"id":"iIuFrdXnNZEC"},"source":["class MultiHeadAttention(tf.keras.layers.Layer):\n","    def __init__(self, d_model, num_heads):\n","        super(MultiHeadAttention, self).__init__()\n","        self.num_heads = num_heads\n","        self.d_model = d_model\n","\n","        assert d_model % self.num_heads == 0\n","\n","        self.depth = d_model // self.num_heads\n","\n","        self.wq = tf.keras.layers.Dense(d_model)\n","        self.wk = tf.keras.layers.Dense(d_model)\n","        self.wv = tf.keras.layers.Dense(d_model)\n","\n","        self.dense = tf.keras.layers.Dense(d_model)\n","        \n","    def split_heads(self, x, batch_size):\n","        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n","        return tf.transpose(x, perm=[0, 2, 1, 3])\n","    \n","    def call(self, v, k, q, mask):\n","        batch_size = tf.shape(q)[0]\n","\n","        q = self.wq(q)\n","        k = self.wk(k)\n","        v = self.wv(v)\n","\n","        q = self.split_heads(q, batch_size)\n","        k = self.split_heads(k, batch_size)\n","        v = self.split_heads(v, batch_size)\n","\n","        scaled_attention, attention_weights = scaled_dot_product_attention(\n","            q, k, v, mask)\n","\n","        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n","\n","        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n","        output = self.dense(concat_attention)\n","            \n","        return output, attention_weights"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A49tXMVvOkOZ"},"source":["### Feed Forward Network"]},{"cell_type":"code","metadata":{"id":"d9-qoKuTNwKq"},"source":["def point_wise_feed_forward_network(d_model, dff): # dff is no of neurons in the layer\n","    return tf.keras.Sequential([\n","        tf.keras.layers.Dense(dff, activation='relu'),\n","        tf.keras.layers.Dense(d_model)\n","    ])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B2RRmn2bOpW9"},"source":["#### Fundamental Unit of Transformer encoder"]},{"cell_type":"code","metadata":{"id":"HNuoJoFWO335"},"source":["class EncoderLayer(tf.keras.layers.Layer):\n","    def __init__(self, d_model, num_heads, dff, rate=0.1):\n","        super(EncoderLayer, self).__init__()\n","\n","        self.mha = MultiHeadAttention(d_model, num_heads)\n","        self.ffn = point_wise_feed_forward_network(d_model, dff)\n","\n","        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","\n","        self.dropout1 = tf.keras.layers.Dropout(rate)\n","        self.dropout2 = tf.keras.layers.Dropout(rate)\n","    \n","    def call(self, x, training, mask):\n","        attn_output, _ = self.mha(x, x, x, mask)\n","        attn_output = self.dropout1(attn_output, training=training)\n","        out1 = self.layernorm1(x + attn_output)\n","\n","        ffn_output = self.ffn(out1)\n","        ffn_output = self.dropout2(ffn_output, training=training)\n","        out2 = self.layernorm2(out1 + ffn_output)\n","\n","        return out2\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9i6Zh8gnPqdW"},"source":["#### Fundamental Unit of Transformer decoder"]},{"cell_type":"code","metadata":{"id":"7CVmvs6dPMRC"},"source":["class DecoderLayer(tf.keras.layers.Layer):\n","    def __init__(self, d_model, num_heads, dff, rate=0.1):\n","        super(DecoderLayer, self).__init__()\n","\n","        self.mha1 = MultiHeadAttention(d_model, num_heads)\n","        self.mha2 = MultiHeadAttention(d_model, num_heads)\n","\n","        self.ffn = point_wise_feed_forward_network(d_model, dff)\n","\n","        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","\n","        self.dropout1 = tf.keras.layers.Dropout(rate)\n","        self.dropout2 = tf.keras.layers.Dropout(rate)\n","        self.dropout3 = tf.keras.layers.Dropout(rate)\n","    \n","    \n","    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n","        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n","        attn1 = self.dropout1(attn1, training=training)\n","        out1 = self.layernorm1(attn1 + x)\n","\n","        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n","        attn2 = self.dropout2(attn2, training=training)\n","        out2 = self.layernorm2(attn2 + out1)\n","\n","        ffn_output = self.ffn(out2)\n","        ffn_output = self.dropout3(ffn_output, training=training)\n","        out3 = self.layernorm3(ffn_output + out2)\n","\n","        return out3, attn_weights_block1, attn_weights_block2\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6zt5MUc_QNid"},"source":["#### Encoder consisting of multiple EncoderLayer(s)"]},{"cell_type":"code","metadata":{"id":"BrbnTwijQJ-h"},"source":["class Encoder(tf.keras.layers.Layer):\n","    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n","        super(Encoder, self).__init__()\n","\n","        self.d_model = d_model\n","        self.num_layers = num_layers\n","\n","        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n","        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n","\n","        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n","\n","        self.dropout = tf.keras.layers.Dropout(rate)\n","        \n","    def call(self, x, training, mask):\n","        seq_len = tf.shape(x)[1]\n","\n","        x = self.embedding(x)\n","        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","        x += self.pos_encoding[:, :seq_len, :]\n","\n","        x = self.dropout(x, training=training)\n","    \n","        for i in range(self.num_layers):\n","            x = self.enc_layers[i](x, training, mask)\n","    \n","        return x\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4N5LrNrvRexg"},"source":["#### Decoder consisting of multiple DecoderLayer(s)"]},{"cell_type":"code","metadata":{"id":"UmeqkZrIRbSB"},"source":["class Decoder(tf.keras.layers.Layer):\n","    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):\n","        super(Decoder, self).__init__()\n","\n","        self.d_model = d_model\n","        self.num_layers = num_layers\n","\n","        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n","        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n","\n","        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n","        self.dropout = tf.keras.layers.Dropout(rate)\n","    \n","    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n","        seq_len = tf.shape(x)[1]\n","        attention_weights = {}\n","\n","        x = self.embedding(x)\n","        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","        x += self.pos_encoding[:, :seq_len, :]\n","\n","        x = self.dropout(x, training=training)\n","\n","        for i in range(self.num_layers):\n","            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n","\n","            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n","            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n","    \n","        return x, attention_weights\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lbMNK_bzSHnh"},"source":["#### Finally, the Transformer"]},{"cell_type":"code","metadata":{"id":"FXHRG-o4R9Mc"},"source":["class Transformer(tf.keras.Model):\n","    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n","        super(Transformer, self).__init__()\n","\n","        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n","\n","        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n","\n","        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n","    \n","    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n","        enc_output = self.encoder(inp, training, enc_padding_mask)\n","\n","        dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n","\n","        final_output = self.final_layer(dec_output)\n","\n","        return final_output, attention_weights\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UndsMPZXTdSr"},"source":["### Training"]},{"cell_type":"code","metadata":{"id":"lMTZJdIoSbuy"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uOGvkYDNTjIj"},"source":["#### Adam optimizer with custom learning rate scheduling"]},{"cell_type":"code","metadata":{"id":"tfiynCLlTL8C"},"source":["class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n","    def __init__(self, d_model, warmup_steps=4000):\n","        super(CustomSchedule, self).__init__()\n","\n","        self.d_model = d_model\n","        self.d_model = tf.cast(self.d_model, tf.float32)\n","\n","        self.warmup_steps = warmup_steps\n","    \n","    def __call__(self, step):\n","        arg1 = tf.math.rsqrt(step)\n","        arg2 = step * (self.warmup_steps ** -1.5)\n","\n","        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DsVdrENTUERY"},"source":["#### Defining losses and other metrics "]},{"cell_type":"code","metadata":{"id":"Ip1-943kTXXK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630939613544,"user_tz":-480,"elapsed":486,"user":{"displayName":"Yuejun Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgS0aPSLsqcw7xL7MdcefrmuNtIFiV2gDoJusqnqg=s64","userId":"03773758233632074141"}},"outputId":"0627318d-7ba7-46bc-fa9b-6b687a64f9d7"},"source":["# hyper-params\n","num_layers = 4\n","d_model = 128\n","dff = 512\n","num_heads = 8\n","EPOCHS = 2\n","\n","learning_rate = CustomSchedule(d_model)\n","print (learning_rate)\n","optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<__main__.CustomSchedule object at 0x7f91cc40f510>\n"]}]},{"cell_type":"code","metadata":{"id":"ktKwyvKtTvF6"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uW4LA_45T4Aa"},"source":["def loss_function(real, pred):\n","    mask = tf.math.logical_not(tf.math.equal(real, 0))\n","    loss_ = loss_object(real, pred)\n","\n","    mask = tf.cast(mask, dtype=loss_.dtype)\n","    loss_ *= mask\n","\n","    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ze0u6xxXT7dI"},"source":["train_loss = tf.keras.metrics.Mean(name='train_loss')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9XvKy3v6ULnO"},"source":["#### Transformer"]},{"cell_type":"code","metadata":{"id":"d5-RcxqFUCuk"},"source":["transformer = Transformer(\n","    num_layers, \n","    d_model, \n","    num_heads, \n","    dff,\n","    encoder_vocab_size, \n","    decoder_vocab_size, \n","    pe_input=encoder_vocab_size, \n","    pe_target=decoder_vocab_size,\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f56BGiVXU_Dk"},"source":["#### Masks"]},{"cell_type":"code","metadata":{"id":"FZxHuyZxU5Pa"},"source":["def create_masks(inp, tar):\n","    enc_padding_mask = create_padding_mask(inp)\n","    dec_padding_mask = create_padding_mask(inp)\n","\n","    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n","    dec_target_padding_mask = create_padding_mask(tar)\n","    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n","  \n","    return enc_padding_mask, combined_mask, dec_padding_mask\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SYIotvaBVI0d"},"source":["#### Checkpoints"]},{"cell_type":"code","metadata":{"id":"tOc1_3c-VGaL"},"source":["checkpoint_path = \"checkpoints\"\n","\n","ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n","\n","ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n","\n","if ckpt_manager.latest_checkpoint:\n","    ckpt.restore(ckpt_manager.latest_checkpoint)\n","    print ('Latest checkpoint restored!!')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WfpI0gS4c06c"},"source":["#### Training steps"]},{"cell_type":"code","metadata":{"id":"xmVOMzkrczgl"},"source":["@tf.function\n","def train_step(inp, tar):\n","    tar_inp = tar[:, :-1]\n","    tar_real = tar[:, 1:]\n","\n","    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n","\n","    with tf.GradientTape() as tape:\n","        predictions, _ = transformer(\n","            inp, tar_inp, \n","            True, \n","            enc_padding_mask, \n","            combined_mask, \n","            dec_padding_mask\n","        )\n","        loss = loss_function(tar_real, predictions)\n","\n","    gradients = tape.gradient(loss, transformer.trainable_variables)    \n","    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n","\n","    train_loss(loss)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xORKpv69dSW5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630757341246,"user_tz":-480,"elapsed":1517592,"user":{"displayName":"Yuejun Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgS0aPSLsqcw7xL7MdcefrmuNtIFiV2gDoJusqnqg=s64","userId":"03773758233632074141"}},"outputId":"90562357-7da7-4311-d7dc-d5bec2dfaece"},"source":["for epoch in range(EPOCHS):\n","    start = time.time()\n","\n","    train_loss.reset_states()\n","  \n","    for (batch, (inp, tar)) in enumerate(dataset):\n","        train_step(inp, tar)\n","    \n","        # 55k samples\n","        # we display 3 batch results -- 0th, middle and last one (approx)\n","        # 55k / 64 ~ 858; 858 / 2 = 429\n","        if batch % 429 == 0:\n","            print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, train_loss.result()))\n","      \n","    if (epoch + 1) % 5 == 0:\n","        ckpt_save_path = ckpt_manager.save()\n","        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1, ckpt_save_path))\n","    \n","    print ('Epoch {} Loss {:.4f}'.format(epoch + 1, train_loss.result()))\n","\n","    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1 Batch 0 Loss 6.3734\n","Epoch 1 Batch 429 Loss 6.2202\n","Epoch 1 Batch 858 Loss 6.0907\n","Epoch 1 Loss 6.0901\n","Time taken for 1 epoch: 759.0987892150879 secs\n","\n","Epoch 2 Batch 0 Loss 5.8798\n","Epoch 2 Batch 429 Loss 5.6857\n","Epoch 2 Batch 858 Loss 5.5763\n","Epoch 2 Loss 5.5759\n","Time taken for 1 epoch: 757.8264200687408 secs\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"PVbEUCZagJ0G"},"source":["### Inference"]},{"cell_type":"markdown","metadata":{"id":"YMbqGTixu1cl"},"source":["#### Predicting one word at a time at the decoder and appending it to the output; then taking the complete sequence as an input to the decoder and repeating until maxlen or stop keyword appears"]},{"cell_type":"code","metadata":{"id":"F5D5cv2Jd8-6"},"source":["def evaluate(input_document):\n","    input_document = document_tokenizer.texts_to_sequences([input_document])\n","    input_document = tf.keras.preprocessing.sequence.pad_sequences(input_document, maxlen=encoder_maxlen, padding='post', truncating='post')\n","\n","    encoder_input = tf.expand_dims(input_document[0], 0)\n","\n","    decoder_input = [summary_tokenizer.word_index[\"<go>\"]]\n","    output = tf.expand_dims(decoder_input, 0)\n","    \n","    for i in range(decoder_maxlen):\n","        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)\n","\n","        predictions, attention_weights = transformer(\n","            encoder_input, \n","            output,\n","            False,\n","            enc_padding_mask,\n","            combined_mask,\n","            dec_padding_mask\n","        )\n","\n","        predictions = predictions[: ,-1:, :]\n","        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n","\n","        if predicted_id == summary_tokenizer.word_index[\"<stop>\"]:\n","            return tf.squeeze(output, axis=0), attention_weights\n","\n","        output = tf.concat([output, predicted_id], axis=-1)\n","\n","    return tf.squeeze(output, axis=0), attention_weights\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UkpdiW6wnmiS"},"source":["def summarize(input_document):\n","    # not considering attention weights for now, can be used to plot attention heatmaps in the future\n","    summarized = evaluate(input_document=input_document)[0].numpy()\n","    summarized = np.expand_dims(summarized[1:], 0)  # not printing <go> token\n","    return summary_tokenizer.sequences_to_texts(summarized)[0]  # since there is just one translated document"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WZoEHvIxrYKZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630757487504,"user_tz":-480,"elapsed":3760,"user":{"displayName":"Yuejun Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgS0aPSLsqcw7xL7MdcefrmuNtIFiV2gDoJusqnqg=s64","userId":"03773758233632074141"}},"outputId":"1c9e02c7-6b95-40c9-c945-75f64f9c7dcf"},"source":["ans = summarize(\n","    \"US-based private equity firm General Atlantic is in talks to invest about \\\n","    $850 million to $950 million in Reliance Industries' digital unit Jio \\\n","    Platforms, the Bloomberg reported. Saudi Arabia's $320 billion sovereign \\\n","    wealth fund is reportedly also exploring a potential investment in the \\\n","    Mukesh Ambani-led company. The 'Public Investment Fund' is looking to \\\n","    acquire a minority stake in Jio Platforms.\"\n",")\n","b = datetime.datetime.now()\n","print(b-a)\n","# For just 2 epochs\n","print(ans)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1:00:16.117523\n","tata ceo to buy its ceo ceo to raise ₹1 crore\n"]}]},{"cell_type":"code","metadata":{"id":"2IJwWWVjU66t","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630757524020,"user_tz":-480,"elapsed":2693,"user":{"displayName":"Yuejun Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgS0aPSLsqcw7xL7MdcefrmuNtIFiV2gDoJusqnqg=s64","userId":"03773758233632074141"}},"outputId":"951910ce-4306-4c30-f78d-9614e4aef1a5"},"source":["ans_1 = summarize(\"One day after Biden clinched enough states to win the presidency, Trump gave no sign of conceding and many of his Republican allies in Congress likewise did not acknowledge Biden’s victory in last Tuesday’s election. \\\n","Instead, Trump will hold a series of rallies to build support for the legal fights challenging the outcome, campaign spokesman Tim Murtaugh confirmed on Sunday. Trump also announced teams to pursue recounts in several states \\\n","and will seek to back up his unfounded accusations of voting fraud by highlighting obituaries of dead people the campaign said voted in the election.\")\n","print(ans_1)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["trump to be a first to be in us prez prez\n"]}]},{"cell_type":"code","metadata":{"id":"vggKof1Tqv9f"},"source":["ans_2 = summarize(\"President Trump briefly acknowledged losing the election, then backtracked, saying he concedes nothing, and vowing to keep up a court fight that election-law experts say is unlikely to succeed.\")\n","print(ans_2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xm2ImNGLyayh"},"source":["**Workshop Submission:**\n","Use this transformer model to translate [SPANISH,English]\n","\n","For example:\n","\n","SPA: hace mucho frio aqui .\n","\n","ENG: it s too hot . \n"]},{"cell_type":"markdown","metadata":{"id":"ZeE0WERMvLjq"},"source":["**Using same model for translation**"]},{"cell_type":"code","metadata":{"id":"2ulxJVeQuXFb"},"source":["# Converts the unicode file to ascii\n","def unicode_to_ascii(s):\n","  return ''.join(c for c in unicodedata.normalize('NFD', s)\n","      if unicodedata.category(c) != 'Mn')\n","\n","\n","def preprocess_sentence(w):\n","  w = unicode_to_ascii(w.lower().strip())\n","\n","  # creating a space between a word and the punctuation following it\n","  # eg: \"he is a boy.\" => \"he is a boy .\"\n","  # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n","  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n","  w = re.sub(r'[\" \"]+', \" \", w)\n","\n","  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n","  w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n","\n","  w = w.strip()\n","\n","  # adding a start and an end token to the sentence\n","  # so that the model know when to start and stop predicting.\n","  w = '<go> ' + w + ' <stop>'\n","  return w\n","\n","# 1. Remove the accents\n","# 2. Clean the sentences\n","# 3. Return word pairs in the format: [ENGLISH, SPANISH]\n","def create_dataset(path, num_examples):\n","  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n","\n","  word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n","\n","  return zip(*word_pairs)\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N5lseaQbzXgJ","executionInfo":{"status":"ok","timestamp":1630938988754,"user_tz":-480,"elapsed":1817,"user":{"displayName":"Yuejun Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgS0aPSLsqcw7xL7MdcefrmuNtIFiV2gDoJusqnqg=s64","userId":"03773758233632074141"}},"outputId":"d9d5a1f6-5f4d-4d80-b15d-2e02b5fc3a0b"},"source":["en, sp = create_dataset(\"spa.txt\", 30000)\n","print(en[-1])\n","print(sp[-1])\n","print(len(en))\n","print(len(sp))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<go> just a moment , please . <stop>\n","<go> un momento , por favor . <stop>\n","30000\n","30000\n"]}]},{"cell_type":"code","metadata":{"id":"lkRrs5C6019u"},"source":["spa_tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=oov_token)\n","eng_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=filters, oov_token=oov_token)\n","\n","spa_tokenizer.fit_on_texts(sp)  # customized tokenizer\n","eng_tokenizer.fit_on_texts(en)  # customized tokenizer, different from input\n","\n","# Use text_to_sequence to obtain the sequence embedding\n","# which is simply a list of word embedding\n","spa_inputs = spa_tokenizer.texts_to_sequences(sp)\n","eng_targets = eng_tokenizer.texts_to_sequences(en)\n","\n","spa_vocab_size = len(spa_tokenizer.word_index) + 1\n","eng_vocab_size = len(eng_tokenizer.word_index) + 1\n","\n","sp_lengths = pd.Series([len(x) for x in sp])  # \n","en_lengths = pd.Series([len(x) for x in en])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FhIQRIppMmJS","executionInfo":{"status":"ok","timestamp":1630938998376,"user_tz":-480,"elapsed":393,"user":{"displayName":"Yuejun Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgS0aPSLsqcw7xL7MdcefrmuNtIFiV2gDoJusqnqg=s64","userId":"03773758233632074141"}},"outputId":"62648d05-a85e-4899-8dc9-ba16a4761231"},"source":["sp_lengths.describe()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["count    30000.000000\n","mean        32.356500\n","std          5.322802\n","min         16.000000\n","25%         29.000000\n","50%         32.000000\n","75%         36.000000\n","max         82.000000\n","dtype: float64"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-9ZboNsNNClj","executionInfo":{"status":"ok","timestamp":1630939001261,"user_tz":-480,"elapsed":430,"user":{"displayName":"Yuejun Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgS0aPSLsqcw7xL7MdcefrmuNtIFiV2gDoJusqnqg=s64","userId":"03773758233632074141"}},"outputId":"8af1def7-7ae6-4e94-9a5a-2a926a8e790f"},"source":["en_lengths.describe()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["count    30000.000000\n","mean        30.462533\n","std          3.295303\n","min         16.000000\n","25%         28.000000\n","50%         31.000000\n","75%         33.000000\n","max         37.000000\n","dtype: float64"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","metadata":{"id":"T8CEPVz2NC7a"},"source":["spa_encoder_maxlen = 82\n","eng_decoder_maxlen = 40"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kz-yRvQANy0E"},"source":["spa_inputs = tf.keras.preprocessing.sequence.pad_sequences(spa_inputs, maxlen=spa_encoder_maxlen, padding='post', truncating='post')\n","eng_targets = tf.keras.preprocessing.sequence.pad_sequences(eng_targets, maxlen=eng_decoder_maxlen, padding='post', truncating='post')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7KUSAQa9ODIk"},"source":["spa_inputs = tf.cast(spa_inputs, dtype=tf.int32)\n","eng_targets = tf.cast(eng_targets, dtype=tf.int32)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HFyUTJk_OOU9"},"source":["BUFFER_SIZE = 20000  # should be set at least equal to the size of the dataset\n","BATCH_SIZE = 64\n","translation_dataset = tf.data.Dataset.from_tensor_slices((spa_inputs, eng_targets)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BmRr8u-Bj9eC","executionInfo":{"status":"ok","timestamp":1630939380550,"user_tz":-480,"elapsed":290,"user":{"displayName":"Yuejun Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgS0aPSLsqcw7xL7MdcefrmuNtIFiV2gDoJusqnqg=s64","userId":"03773758233632074141"}},"outputId":"fdc11db9-bc72-482c-ff93-a8b1270ba8d8"},"source":["translation_dataset"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<BatchDataset shapes: ((None, 82), (None, 40)), types: (tf.int32, tf.int32)>"]},"metadata":{},"execution_count":41}]},{"cell_type":"code","metadata":{"id":"hE5OBnpOO5Ut","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630939935558,"user_tz":-480,"elapsed":504,"user":{"displayName":"Yuejun Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgS0aPSLsqcw7xL7MdcefrmuNtIFiV2gDoJusqnqg=s64","userId":"03773758233632074141"}},"outputId":"9e4fa265-882b-482d-e4e4-b02db537372a"},"source":["num_layers = 4\n","d_model = 128\n","dff = 512\n","num_heads = 8\n","EPOCHS = 10\n","learning_rate = CustomSchedule(d_model)\n","print(learning_rate)\n","optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n","\n","\n","transformer_translation = Transformer(\n","    num_layers, \n","    d_model, \n","    num_heads, \n","    dff,\n","    spa_vocab_size, \n","    eng_vocab_size, \n","    pe_input=spa_vocab_size, \n","    pe_target=eng_vocab_size,\n",")\n","\n","checkpoint_path = \"checkpoints\"\n","\n","ckpt = tf.train.Checkpoint(transformer=transformer_translation, optimizer=optimizer)\n","\n","ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n","\n","if ckpt_manager.latest_checkpoint:\n","    ckpt.restore(ckpt_manager.latest_checkpoint)\n","    print ('Latest checkpoint restored!!')\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<__main__.CustomSchedule object at 0x7f91cc671c50>\n"]}]},{"cell_type":"code","metadata":{"id":"gF7UtNF0l4bq"},"source":["@tf.function\n","def train_step(inp, tar):\n","    tar_inp = tar[:, :-1]\n","    tar_real = tar[:, 1:]\n","\n","    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n","\n","    with tf.GradientTape() as tape:\n","        predictions, _ = transformer_translation(\n","            inp, tar_inp, \n","            True, \n","            enc_padding_mask, \n","            combined_mask, \n","            dec_padding_mask\n","        )\n","        loss = loss_function(tar_real, predictions)\n","\n","    gradients = tape.gradient(loss, transformer_translation.trainable_variables)    \n","    optimizer.apply_gradients(zip(gradients, transformer_translation.trainable_variables))\n","\n","    train_loss(loss)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hKbJ2w3nlS0b","executionInfo":{"status":"ok","timestamp":1630940767627,"user_tz":-480,"elapsed":779074,"user":{"displayName":"Yuejun Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgS0aPSLsqcw7xL7MdcefrmuNtIFiV2gDoJusqnqg=s64","userId":"03773758233632074141"}},"outputId":"b06d60eb-daa1-4926-a3fa-6ed60f20ae6f"},"source":["\n","for epoch in range(EPOCHS):\n","    start = time.time()\n","\n","    train_loss.reset_states()\n","  \n","    for (batch, (inp, tar)) in enumerate(translation_dataset):\n","        train_step(inp, tar)\n","    \n","        # 30000 samples\n","        # we display 3 batch results -- 0th, middle and last one (approx)\n","        # 30000 / 64 ~ 468; 468 / 2 = 234\n","        if batch % 234 == 0:\n","            print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, train_loss.result()))\n","      \n","    if (epoch + 1) % 5 == 0:\n","        ckpt_save_path = ckpt_manager.save()\n","        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1, ckpt_save_path))\n","    \n","    print ('Epoch {} Loss {:.4f}'.format(epoch + 1, train_loss.result()))\n","\n","    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1 Batch 0 Loss 7.0549\n","Epoch 1 Batch 234 Loss 5.9777\n","Epoch 1 Batch 468 Loss 5.2688\n","Epoch 1 Loss 5.2688\n","Time taken for 1 epoch: 81.92670345306396 secs\n","\n","Epoch 2 Batch 0 Loss 3.9138\n","Epoch 2 Batch 234 Loss 3.7361\n","Epoch 2 Batch 468 Loss 3.5610\n","Epoch 2 Loss 3.5610\n","Time taken for 1 epoch: 68.00685930252075 secs\n","\n","Epoch 3 Batch 0 Loss 3.0049\n","Epoch 3 Batch 234 Loss 2.9679\n","Epoch 3 Batch 468 Loss 2.9030\n","Epoch 3 Loss 2.9030\n","Time taken for 1 epoch: 81.91672563552856 secs\n","\n","Epoch 4 Batch 0 Loss 2.6774\n","Epoch 4 Batch 234 Loss 2.5070\n","Epoch 4 Batch 468 Loss 2.4432\n","Epoch 4 Loss 2.4432\n","Time taken for 1 epoch: 81.91685557365417 secs\n","\n","Epoch 5 Batch 0 Loss 2.1712\n","Epoch 5 Batch 234 Loss 2.0440\n","Epoch 5 Batch 468 Loss 2.0168\n","Saving checkpoint for epoch 5 at checkpoints/ckpt-1\n","Epoch 5 Loss 2.0168\n","Time taken for 1 epoch: 68.62476539611816 secs\n","\n","Epoch 6 Batch 0 Loss 1.6559\n","Epoch 6 Batch 234 Loss 1.6731\n","Epoch 6 Batch 468 Loss 1.6712\n","Epoch 6 Loss 1.6712\n","Time taken for 1 epoch: 67.99343729019165 secs\n","\n","Epoch 7 Batch 0 Loss 1.3512\n","Epoch 7 Batch 234 Loss 1.3851\n","Epoch 7 Batch 468 Loss 1.4225\n","Epoch 7 Loss 1.4225\n","Time taken for 1 epoch: 81.91335272789001 secs\n","\n","Epoch 8 Batch 0 Loss 1.1988\n","Epoch 8 Batch 234 Loss 1.2024\n","Epoch 8 Batch 468 Loss 1.2594\n","Epoch 8 Loss 1.2594\n","Time taken for 1 epoch: 81.91923904418945 secs\n","\n","Epoch 9 Batch 0 Loss 0.9099\n","Epoch 9 Batch 234 Loss 1.0926\n","Epoch 9 Batch 468 Loss 1.1267\n","Epoch 9 Loss 1.1267\n","Time taken for 1 epoch: 81.91451358795166 secs\n","\n","Epoch 10 Batch 0 Loss 0.9933\n","Epoch 10 Batch 234 Loss 0.9477\n","Epoch 10 Batch 468 Loss 0.9807\n","Saving checkpoint for epoch 10 at checkpoints/ckpt-2\n","Epoch 10 Loss 0.9807\n","Time taken for 1 epoch: 82.41958379745483 secs\n","\n"]}]},{"cell_type":"code","metadata":{"id":"38XQaVK7PWp2"},"source":["def evaluate(input_document):\n","    input_document = spa_tokenizer.texts_to_sequences([input_document])\n","    input_document = tf.keras.preprocessing.sequence.pad_sequences(input_document, maxlen=spa_encoder_maxlen, padding='post', truncating='post')\n","\n","    encoder_input = tf.expand_dims(input_document[0], 0)\n","\n","    decoder_input = [eng_tokenizer.word_index[\"<go>\"]]\n","    output = tf.expand_dims(decoder_input, 0)\n","    \n","    for i in range(eng_decoder_maxlen):\n","        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)\n","\n","        predictions, attention_weights = transformer_translation(\n","            encoder_input, \n","            output,\n","            False,\n","            enc_padding_mask,\n","            combined_mask,\n","            dec_padding_mask\n","        )\n","\n","        predictions = predictions[: ,-1:, :]\n","        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n","\n","        if predicted_id == eng_tokenizer.word_index[\"<stop>\"]:\n","            return tf.squeeze(output, axis=0), attention_weights\n","\n","        output = tf.concat([output, predicted_id], axis=-1)\n","\n","    return tf.squeeze(output, axis=0), attention_weights\n","\n","def translate(input_document):\n","    # not considering attention weights for now, can be used to plot attention heatmaps in the future\n","    translated = evaluate(input_document=input_document)[0].numpy()\n","    translated = np.expand_dims(translated[1:], 0)  # not printing <go> token\n","    return eng_tokenizer.sequences_to_texts(translated)[0]  # since there is just one translated document"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x01iKmYGPiIW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630942167708,"user_tz":-480,"elapsed":1269,"user":{"displayName":"Yuejun Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgS0aPSLsqcw7xL7MdcefrmuNtIFiV2gDoJusqnqg=s64","userId":"03773758233632074141"}},"outputId":"dd2560f0-46d5-484f-b4f9-dbe3fb5f31ea"},"source":["q1 = 'hace mucho frio aqui .'\n","\n","print(translate(q1))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["it s cold hard here\n"]}]},{"cell_type":"markdown","metadata":{"id":"DEjDFi_Curbi"},"source":["**translation is close but exactly opposite, since we only used 30k samples as training, it might not learn properly**"]}]}